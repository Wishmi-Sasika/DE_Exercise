{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "executionInfo": {
          "elapsed": 614,
          "status": "ok",
          "timestamp": 1756036248609,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -330
        },
        "id": "6v7Sb9DmxMkd"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import random\n",
        "import requests\n",
        "import pandas as pd\n",
        "from datetime import datetime, timedelta\n",
        "from typing import Dict, Any\n",
        "\n",
        "from google.cloud import bigquery\n",
        "from google.cloud.bigquery import LoadJobConfig, SourceFormat, WriteDisposition\n",
        "from google.api_core.exceptions import NotFound"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "executionInfo": {
          "elapsed": 5,
          "status": "ok",
          "timestamp": 1756036249176,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -330
        },
        "id": "RYFRh5nfzfjK"
      },
      "outputs": [],
      "source": [
        "# Config\n",
        "PROJECT_ID = os.getenv(\"BQ_PROJECT\", \"analytics-pipeline-assessment\")\n",
        "DATASET_ID = os.getenv(\"BQ_DATASET\", \"analytics_dw\")\n",
        "bq = bigquery.Client(project=PROJECT_ID)\n",
        "\n",
        "def log(step: str, **kwargs):\n",
        "    print(json.dumps({\"ts\": datetime.utcnow().isoformat()+\"Z\", \"step\": step, **kwargs}))\n",
        "\n",
        "def run_sql(sql: str):\n",
        "    return bq.query(sql, job_id_prefix=\"nbq_\").result()\n",
        "\n",
        "def ensure_dataset():\n",
        "    ds = f\"{PROJECT_ID}.{DATASET_ID}\"\n",
        "    try:\n",
        "        bq.get_dataset(ds)\n",
        "        log(\"dataset.exists\", dataset=ds)\n",
        "    except Exception:\n",
        "        bq.create_dataset(bigquery.Dataset(ds), exists_ok=True)\n",
        "        log(\"dataset.created\", dataset=ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X2jOHdZwwhJY"
      },
      "source": [
        "Attendance generator function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "executionInfo": {
          "elapsed": 4,
          "status": "ok",
          "timestamp": 1756036249176,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -330
        },
        "id": "pk894KKR5h3k"
      },
      "outputs": [],
      "source": [
        "def gen_attendance(num_records: int = 3_000_000,\n",
        "                   output_file: str = \"attendance_dataset_3m.csv\") -> Dict[str, Any]:\n",
        "\n",
        "    regions = [\"North America\", \"Europe\", \"Asia\", \"South America\", \"Africa\", \"Oceania\"]\n",
        "    countries = {\n",
        "        \"North America\": [\"USA\", \"Canada\", \"Mexico\"],\n",
        "        \"Europe\": [\"Germany\", \"France\", \"UK\", \"Italy\"],\n",
        "        \"Asia\": [\"China\", \"India\", \"Japan\", \"Singapore\"],\n",
        "        \"South America\": [\"Brazil\", \"Argentina\", \"Chile\"],\n",
        "        \"Africa\": [\"South Africa\", \"Nigeria\", \"Egypt\"],\n",
        "        \"Oceania\": [\"Australia\", \"New Zealand\"]\n",
        "    }\n",
        "    departments = [\"IT\", \"Sales\", \"Marketing\", \"HR\", \"Finance\", \"Operations\"]\n",
        "    first_names = [\"Alice\", \"Bob\", \"Chen\", \"Daniela\", \"Ethan\", \"Fatima\", \"George\", \"Hiro\", \"Isabella\", \"Juan\"]\n",
        "    last_names = [\"Johnson\", \"Smith\", \"Wei\", \"Lopez\", \"Brown\", \"Hassan\", \"Wilson\", \"Tanaka\", \"Rossi\", \"Martinez\"]\n",
        "    statuses = [\"Present\", \"Absent\", \"Remote\"]\n",
        "\n",
        "    def generate_attendance_data(n):\n",
        "        for i in range(1, n+1):\n",
        "            staff_id = f\"ST{i:07d}\"\n",
        "            name = f\"{random.choice(first_names)} {random.choice(last_names)}\"\n",
        "            region = random.choice(regions)\n",
        "            country = random.choice(countries[region])\n",
        "            department = random.choice(departments)\n",
        "            date = datetime(2020, 1, 1) + timedelta(days=random.randint(0, 1825))  # 5 years\n",
        "            status = random.choices(statuses, weights=[0.7, 0.1, 0.2])[0]\n",
        "            if status in [\"Present\", \"Remote\"]:\n",
        "                check_in_hour = random.randint(8, 10)\n",
        "                check_in_minute = random.randint(0, 59)\n",
        "                check_out_hour = random.randint(16, 18)\n",
        "                check_out_minute = random.randint(0, 59)\n",
        "                check_in = f\"{check_in_hour:02d}:{check_in_minute:02d}\"\n",
        "                check_out = f\"{check_out_hour:02d}:{check_out_minute:02d}\"\n",
        "            else:\n",
        "                check_in, check_out = \"-\", \"-\"\n",
        "\n",
        "            yield [\n",
        "                staff_id, name, region, country, department, date.strftime(\"%Y-%m-%d\"),\n",
        "                status, check_in, check_out\n",
        "            ]\n",
        "\n",
        "    # Write CSV in chunks\n",
        "    columns = [\"StaffID\", \"Name\", \"Region\", \"Country\", \"Department\", \"Date\",\n",
        "               \"Status\", \"CheckInTime\", \"CheckOutTime\"]\n",
        "    chunk_size = 100_000\n",
        "\n",
        "    log(\"gen.attendance.start\", records=num_records, output=output_file)\n",
        "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(\",\".join(columns) + \"\\n\")\n",
        "        for start in range(0, num_records, chunk_size):\n",
        "            chunk = list(generate_attendance_data(min(chunk_size, num_records - start)))\n",
        "            df = pd.DataFrame(chunk, columns=columns)\n",
        "            df.to_csv(f, header=False, index=False)\n",
        "    print(f\"✅ Attendance dataset generated: {output_file}\")\n",
        "\n",
        "    log(\"gen.attendance.done\", file=output_file)\n",
        "\n",
        "    return {\"type\": \"path\", \"data\": output_file}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LXy1nZB0xxB5"
      },
      "source": [
        "Sales data generator function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "executionInfo": {
          "elapsed": 5,
          "status": "ok",
          "timestamp": 1756036249177,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -330
        },
        "id": "QL6YnFkA9K1n"
      },
      "outputs": [],
      "source": [
        "def gen_sales(num_records: int = 3_000_000,\n",
        "              output_file: str = \"sales_dataset_3m.csv\") -> Dict[str, Any]:\n",
        "\n",
        "    regions = [\"North America\", \"Europe\", \"Asia\", \"South America\", \"Africa\", \"Oceania\"]\n",
        "    countries = {\n",
        "        \"North America\": [\"USA\", \"Canada\", \"Mexico\"],\n",
        "        \"Europe\": [\"Germany\", \"France\", \"UK\", \"Italy\"],\n",
        "        \"Asia\": [\"China\", \"India\", \"Japan\", \"Singapore\"],\n",
        "        \"South America\": [\"Brazil\", \"Argentina\", \"Chile\"],\n",
        "        \"Africa\": [\"South Africa\", \"Nigeria\", \"Egypt\"],\n",
        "        \"Oceania\": [\"Australia\", \"New Zealand\"]\n",
        "    }\n",
        "    currencies = {\n",
        "        \"USA\": \"USD\", \"Canada\": \"CAD\", \"Mexico\": \"MXN\",\n",
        "        \"Germany\": \"EUR\", \"France\": \"EUR\", \"UK\": \"GBP\", \"Italy\": \"EUR\",\n",
        "        \"China\": \"CNY\", \"India\": \"INR\", \"Japan\": \"JPY\", \"Singapore\": \"SGD\",\n",
        "        \"Brazil\": \"BRL\", \"Argentina\": \"ARS\", \"Chile\": \"CLP\",\n",
        "        \"South Africa\": \"ZAR\", \"Nigeria\": \"NGN\", \"Egypt\": \"EGP\",\n",
        "        \"Australia\": \"AUD\", \"New Zealand\": \"NZD\"\n",
        "    }\n",
        "    products = [\"Software\", \"Hardware\", \"Consulting\", \"Cloud Services\", \"Licenses\"]\n",
        "\n",
        "    def generate_sales_data(n):\n",
        "        for i in range(1, n+1):\n",
        "            region = random.choice(regions)\n",
        "            country = random.choice(countries[region])\n",
        "            product = random.choice(products)\n",
        "            currency = currencies[country]\n",
        "            date = datetime(2020, 1, 1) + timedelta(days=random.randint(0, 1825))  # 5 years\n",
        "            quantity = random.randint(1, 50)\n",
        "            unit_price = round(random.uniform(100, 5000), 2)\n",
        "            total_sales = round(quantity * unit_price, 2)\n",
        "\n",
        "            yield [\n",
        "                f\"S{i:07d}\", region, country, product, date.strftime(\"%Y-%m-%d\"),\n",
        "                currency, quantity, unit_price, total_sales\n",
        "            ]\n",
        "\n",
        "    columns = [\"SaleID\", \"Region\", \"Country\", \"Product\", \"Date\",\n",
        "               \"Currency\", \"Quantity\", \"UnitPrice\", \"TotalSales\"]\n",
        "    chunk_size = 100_000\n",
        "\n",
        "    log(\"gen.sales.start\", records=num_records, output=output_file)\n",
        "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(\",\".join(columns) + \"\\n\")\n",
        "        for start in range(0, num_records, chunk_size):\n",
        "            chunk = list(generate_sales_data(min(chunk_size, num_records - start)))\n",
        "            df = pd.DataFrame(chunk, columns=columns)\n",
        "            df.to_csv(f, header=False, index=False)\n",
        "    print(f\"✅ Sales dataset generated: {output_file}\")\n",
        "\n",
        "    log(\"gen.sales.done\", file=output_file)\n",
        "\n",
        "    return {\"type\": \"path\", \"data\": output_file}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AuIqrLNHyKNX"
      },
      "source": [
        "Finance Data Generator function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "executionInfo": {
          "elapsed": 5,
          "status": "ok",
          "timestamp": 1756036249177,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -330
        },
        "id": "IkbcZJBm9Ky-"
      },
      "outputs": [],
      "source": [
        "def gen_finance(num_records: int = 3_000_000,\n",
        "                output_file: str = \"financial_dataset_3m.csv\") -> Dict[str, Any]:\n",
        "\n",
        "    regions = [\"North America\", \"Europe\", \"Asia\", \"South America\", \"Africa\", \"Oceania\"]\n",
        "    countries = {\n",
        "        \"North America\": [\"USA\", \"Canada\", \"Mexico\"],\n",
        "        \"Europe\": [\"Germany\", \"France\", \"UK\", \"Italy\"],\n",
        "        \"Asia\": [\"China\", \"India\", \"Japan\", \"Singapore\"],\n",
        "        \"South America\": [\"Brazil\", \"Argentina\", \"Chile\"],\n",
        "        \"Africa\": [\"South Africa\", \"Nigeria\", \"Egypt\"],\n",
        "        \"Oceania\": [\"Australia\", \"New Zealand\"]\n",
        "    }\n",
        "    currencies = {\n",
        "        \"USA\": \"USD\", \"Canada\": \"CAD\", \"Mexico\": \"MXN\",\n",
        "        \"Germany\": \"EUR\", \"France\": \"EUR\", \"UK\": \"GBP\", \"Italy\": \"EUR\",\n",
        "        \"China\": \"CNY\", \"India\": \"INR\", \"Japan\": \"JPY\", \"Singapore\": \"SGD\",\n",
        "        \"Brazil\": \"BRL\", \"Argentina\": \"ARS\", \"Chile\": \"CLP\",\n",
        "        \"South Africa\": \"ZAR\", \"Nigeria\": \"NGN\", \"Egypt\": \"EGP\",\n",
        "        \"Australia\": \"AUD\", \"New Zealand\": \"NZD\"\n",
        "    }\n",
        "    products = [\"Software\", \"Hardware\", \"Consulting\", \"Cloud Services\", \"Licenses\"]\n",
        "\n",
        "    def generate_finance_data(n):\n",
        "        for i in range(1, n+1):\n",
        "            region = random.choice(regions)\n",
        "            country = random.choice(countries[region])\n",
        "            currency = currencies[country]\n",
        "            product = random.choice(products)\n",
        "            date = datetime(2020, 1, 1) + timedelta(days=random.randint(0, 1825))  # 5 years\n",
        "            revenue = round(random.uniform(1000, 100000), 2)\n",
        "            expense = round(revenue * random.uniform(0.4, 0.9), 2)\n",
        "            profit = revenue - expense\n",
        "\n",
        "            yield [\n",
        "                f\"T{i:07d}\", region, country, product, date.strftime(\"%Y-%m-%d\"),\n",
        "                currency, revenue, expense, profit\n",
        "            ]\n",
        "\n",
        "    columns = [\"TransactionID\", \"Region\", \"Country\", \"Product\", \"Date\",\n",
        "               \"Currency\", \"Revenue\", \"Expense\", \"Profit\"]\n",
        "    chunk_size = 100_000\n",
        "\n",
        "    log(\"gen.finance.start\", records=num_records, output=output_file)\n",
        "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(\",\".join(columns) + \"\\n\")\n",
        "        for start in range(0, num_records, chunk_size):\n",
        "            chunk = list(generate_finance_data(min(chunk_size, num_records - start)))\n",
        "            df = pd.DataFrame(chunk, columns=columns)\n",
        "            df.to_csv(f, header=False, index=False)\n",
        "    print(f\"✅ Finance dataset generated: {output_file}\")\n",
        "\n",
        "    log(\"gen.finance.done\", file=output_file)\n",
        "\n",
        "    return {\"type\": \"path\", \"data\": output_file}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "executionInfo": {
          "elapsed": 5,
          "status": "ok",
          "timestamp": 1756036249177,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -330
        },
        "id": "L1QXW3_Ayssj"
      },
      "outputs": [],
      "source": [
        "def gen_data():\n",
        "\n",
        "    log(\"gen_data.start\", order=\"attendance>sales>finance\")\n",
        "\n",
        "    a_src = gen_attendance()\n",
        "    s_src = gen_sales()\n",
        "    f_src = gen_finance()\n",
        "\n",
        "    log(\"gen_data.done\", outputs=[a_src[\"data\"], s_src[\"data\"], f_src[\"data\"]])\n",
        "    return a_src, s_src, f_src"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "08xhdrRHzs6e"
      },
      "source": [
        "DDL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 5292,
          "status": "ok",
          "timestamp": 1756036254465,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -330
        },
        "id": "4Vu98L0Fysp9",
        "outputId": "a2417516-b351-4623-cc2d-39d3e7ab0d2d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\"ts\": \"2025-08-24T11:50:48.848446Z\", \"step\": \"ddl.applied\"}\n"
          ]
        }
      ],
      "source": [
        "DDL_SQL = f\"\"\"\n",
        "CREATE SCHEMA IF NOT EXISTS `{PROJECT_ID}.{DATASET_ID}`;\n",
        "\n",
        "CREATE TABLE IF NOT EXISTS `{PROJECT_ID}.{DATASET_ID}.dim_date` (\n",
        "  date_key DATE,\n",
        "  year INT64,\n",
        "  quarter INT64,\n",
        "  month INT64,\n",
        "  month_name STRING,\n",
        "  day_of_month INT64,\n",
        "  day_of_week INT64,\n",
        "  day_name STRING\n",
        ");\n",
        "\n",
        "CREATE TABLE IF NOT EXISTS `{PROJECT_ID}.{DATASET_ID}.dim_location` (\n",
        "  location_key STRING,\n",
        "  region STRING,\n",
        "  country STRING\n",
        ");\n",
        "\n",
        "CREATE TABLE IF NOT EXISTS `{PROJECT_ID}.{DATASET_ID}.dim_product` (\n",
        "  product_key STRING,\n",
        "  product_name STRING\n",
        ");\n",
        "\n",
        "CREATE TABLE IF NOT EXISTS `{PROJECT_ID}.{DATASET_ID}.dim_employee` (\n",
        "  employee_key STRING,\n",
        "  staffid STRING,\n",
        "  name STRING,\n",
        "  department STRING,\n",
        "  home_country STRING,\n",
        "  home_region STRING\n",
        ");\n",
        "\n",
        "CREATE TABLE IF NOT EXISTS `{PROJECT_ID}.{DATASET_ID}.dim_currency` (\n",
        "  currency_key STRING,\n",
        "  currency_code STRING,\n",
        "  date_to_usd NUMERIC\n",
        ");\n",
        "\n",
        "CREATE TABLE IF NOT EXISTS `{PROJECT_ID}.{DATASET_ID}.stg_attendance` (\n",
        "  StaffID STRING,\n",
        "  Name STRING,\n",
        "  Region STRING,\n",
        "  Country STRING,\n",
        "  Department STRING,\n",
        "  Date DATE,\n",
        "  Status STRING,\n",
        "  CheckInTime STRING,\n",
        "  CheckOutTime STRING\n",
        ");\n",
        "\n",
        "CREATE TABLE IF NOT EXISTS `{PROJECT_ID}.{DATASET_ID}.stg_sales` (\n",
        "  SaleID STRING,\n",
        "  Region STRING,\n",
        "  Country STRING,\n",
        "  Product STRING,\n",
        "  Date DATE,\n",
        "  Currency STRING,\n",
        "  Quantity INT64,\n",
        "  UnitPrice NUMERIC,\n",
        "  TotalSales NUMERIC\n",
        ");\n",
        "\n",
        "CREATE TABLE IF NOT EXISTS `{PROJECT_ID}.{DATASET_ID}.stg_finance` (\n",
        "  TransactionID STRING,\n",
        "  Region STRING,\n",
        "  Country STRING,\n",
        "  Product STRING,\n",
        "  Date DATE,\n",
        "  Currency STRING,\n",
        "  Revenue NUMERIC,\n",
        "  Expense NUMERIC,\n",
        "  Profit NUMERIC\n",
        ");\n",
        "\n",
        "CREATE TABLE IF NOT EXISTS `{PROJECT_ID}.{DATASET_ID}.fact_attendance` (\n",
        "  attendance_key STRING,\n",
        "  employee_key STRING,\n",
        "  location_key STRING,\n",
        "  date_key DATE,\n",
        "  status STRING,\n",
        "  checkin_time TIMESTAMP,\n",
        "  checkout_time TIMESTAMP\n",
        ");\n",
        "\n",
        "CREATE TABLE IF NOT EXISTS `{PROJECT_ID}.{DATASET_ID}.fact_sales` (\n",
        "  saleid STRING,\n",
        "  product_key STRING,\n",
        "  location_key STRING,\n",
        "  date_key DATE,\n",
        "  currency_key STRING,\n",
        "  quantity INT64,\n",
        "  conversion_rate_to_usd NUMERIC,\n",
        "  unit_price_usd NUMERIC,\n",
        "  total_sales_usd NUMERIC\n",
        ");\n",
        "\n",
        "CREATE TABLE IF NOT EXISTS `{PROJECT_ID}.{DATASET_ID}.fact_finance` (\n",
        "  transaction_id STRING,\n",
        "  product_key STRING,\n",
        "  location_key STRING,\n",
        "  date_key DATE,\n",
        "  currency_key STRING,\n",
        "  conversion_rate_to_usd NUMERIC,\n",
        "  revenue_usd NUMERIC,\n",
        "  expense_usd NUMERIC,\n",
        "  profit_usd NUMERIC\n",
        ");\n",
        "\"\"\"\n",
        "run_sql(DDL_SQL)\n",
        "log(\"ddl.applied\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-h4NC8oc0NdB"
      },
      "source": [
        "Load data into staging tables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "executionInfo": {
          "elapsed": 5,
          "status": "ok",
          "timestamp": 1756036254465,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -330
        },
        "id": "bvJ7e5jXPaVI"
      },
      "outputs": [],
      "source": [
        "# Type coercion map per staging table\n",
        "COERCE_COLUMNS: Dict[str, Dict[str, str]] = {\n",
        "    \"stg_attendance\": {\"Date\": \"date\", \"CheckInTime\": \"time\", \"CheckOutTime\": \"time\", \"StaffID\": \"int\"},\n",
        "    \"stg_sales\":      {\"Date\": \"date\", \"Quantity\": \"int\", \"UnitPrice\": \"float\"},\n",
        "    \"stg_finance\":    {\"Date\": \"date\", \"Revenue\": \"float\", \"Expense\": \"float\", \"Profit\": \"float\"},\n",
        "}\n",
        "\n",
        "# Helpers\n",
        "def _coerce_types_light(df: pd.DataFrame, coerce: Dict[str, str]) -> pd.DataFrame:\n",
        "    out = df.copy()\n",
        "\n",
        "    def _clean_time_series(s: pd.Series) -> pd.Series:\n",
        "        s = (\n",
        "            s.astype(str)\n",
        "             .str.strip()\n",
        "             .replace({\"-\": pd.NA, \"\": pd.NA, \"NULL\": pd.NA, \"NaN\": pd.NA, \"nan\": pd.NA, \"None\": pd.NA})\n",
        "        )\n",
        "        parsed = pd.to_datetime(s, errors=\"coerce\").dt.time \n",
        "        return parsed.where(pd.notna(parsed), None) \n",
        "\n",
        "    for col, kind in (coerce or {}).items():\n",
        "        if col not in out.columns:\n",
        "            continue\n",
        "        if kind == \"date\":\n",
        "            out[col] = pd.to_datetime(out[col], errors=\"coerce\").dt.date\n",
        "        elif kind == \"time\":\n",
        "            out[col] = _clean_time_series(out[col])\n",
        "        elif kind == \"int\":\n",
        "            out[col] = pd.to_numeric(out[col], errors=\"coerce\").astype(\"Int64\")\n",
        "        elif kind == \"float\":\n",
        "            out[col] = pd.to_numeric(out[col], errors=\"coerce\")\n",
        "    return out\n",
        "\n",
        "def _read_src_to_df(src: Dict[str, Any]) -> pd.DataFrame:\n",
        "    if src[\"type\"] == \"df\":\n",
        "        return src[\"data\"].copy()\n",
        "    elif src[\"type\"] == \"path\":\n",
        "        path = src[\"data\"]\n",
        "        if path.lower().endswith(\".csv\"):\n",
        "            return pd.read_csv(path)\n",
        "        else:\n",
        "            return pd.read_json(path, lines=True)\n",
        "    else:\n",
        "        raise ValueError(\"Unknown src type; expected {'type':'df'|'path','data':...}\")\n",
        "\n",
        "def _load_df(table_id: str, df: pd.DataFrame):\n",
        "    job_config = LoadJobConfig(write_disposition=WriteDisposition.WRITE_TRUNCATE)\n",
        "    bq.load_table_from_dataframe(df, table_id, job_config=job_config).result()\n",
        "\n",
        "def _dedupe_and_load_to_bq(table_id: str, src: Dict[str, Any]) -> Dict[str, Any]:\n",
        "    short = table_id.split(\".\")[-1]\n",
        "    rejects_table = table_id + \"_rejects\"\n",
        "\n",
        "    df_in = _read_src_to_df(src)\n",
        "    total_rows = int(len(df_in) if df_in is not None else 0)\n",
        "\n",
        "    # Normalize types before duplicate detection to match DW expectations\n",
        "    df_norm = _coerce_types_light(df_in, COERCE_COLUMNS.get(short, {}))\n",
        "\n",
        "    if total_rows == 0:\n",
        "        # ensure targets exist and empty\n",
        "        try:\n",
        "            bq.query(f\"TRUNCATE TABLE `{table_id}`\").result()\n",
        "        except Exception:\n",
        "            pass\n",
        "        try:\n",
        "            bq.get_table(rejects_table)\n",
        "            bq.query(f\"TRUNCATE TABLE `{rejects_table}`\").result()\n",
        "        except Exception:\n",
        "            pass\n",
        "        log(f\"{short}.validated\", total=0, loaded=0, rejected=0)\n",
        "        return {\"table\": short, \"total_rows\": 0, \"loaded_rows\": 0, \"rejected_rows\": 0, \"rejects_table\": rejects_table}\n",
        "\n",
        "    # Exact-row duplicate detection\n",
        "    dup_mask = df_norm.duplicated(keep=\"first\")\n",
        "    rejects_df = df_norm.loc[dup_mask].copy()\n",
        "    valid_df   = df_norm.loc[~dup_mask].copy()\n",
        "\n",
        "    if not rejects_df.empty:\n",
        "        rejects_df[\"error_reason\"] = \"duplicate_row\"\n",
        "\n",
        "    # Load valid\n",
        "    if not valid_df.empty:\n",
        "        _load_df(table_id, valid_df)\n",
        "        loaded_rows = int(len(valid_df))\n",
        "    else:\n",
        "        try:\n",
        "            bq.query(f\"TRUNCATE TABLE `{table_id}`\").result()\n",
        "        except Exception:\n",
        "            pass\n",
        "        loaded_rows = 0\n",
        "\n",
        "    # Load and clear rejects\n",
        "    if not rejects_df.empty:\n",
        "        _load_df(rejects_table, rejects_df)\n",
        "        rejected_rows = int(len(rejects_df))\n",
        "    else:\n",
        "        try:\n",
        "            bq.get_table(rejects_table)\n",
        "            bq.query(f\"TRUNCATE TABLE `{rejects_table}`\").result()\n",
        "        except Exception:\n",
        "            pass\n",
        "        rejected_rows = 0\n",
        "\n",
        "    log(f\"{short}.validated\", total=total_rows, loaded=loaded_rows, rejected=rejected_rows)\n",
        "    return {\n",
        "        \"table\": short,\n",
        "        \"total_rows\": total_rows,\n",
        "        \"loaded_rows\": loaded_rows,\n",
        "        \"rejected_rows\": rejected_rows,\n",
        "        \"rejects_table\": rejects_table,\n",
        "    }\n",
        "\n",
        "def stage_from_generated(a_src, s_src, f_src):\n",
        "    log(\"staging.start\")\n",
        "\n",
        "    att = _dedupe_and_load_to_bq(f\"{PROJECT_ID}.{DATASET_ID}.stg_attendance\", a_src)\n",
        "    log(\"staging.attendance.done\", rows=att[\"loaded_rows\"], rejected=att[\"rejected_rows\"])\n",
        "\n",
        "    sal = _dedupe_and_load_to_bq(f\"{PROJECT_ID}.{DATASET_ID}.stg_sales\", s_src)\n",
        "    log(\"staging.sales.done\", rows=sal[\"loaded_rows\"], rejected=sal[\"rejected_rows\"])\n",
        "\n",
        "    fin = _dedupe_and_load_to_bq(f\"{PROJECT_ID}.{DATASET_ID}.stg_finance\", f_src)\n",
        "    log(\"staging.finance.done\", rows=fin[\"loaded_rows\"], rejected=fin[\"rejected_rows\"])\n",
        "\n",
        "    log(\"staging.all.done\", totals={\"attendance\": att, \"sales\": sal, \"finance\": fin})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EiNaMqWh015f"
      },
      "source": [
        "Load data into dim tables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "executionInfo": {
          "elapsed": 5,
          "status": "ok",
          "timestamp": 1756036254465,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -330
        },
        "id": "anBroGHF9KwR"
      },
      "outputs": [],
      "source": [
        "def build_dimensions():\n",
        "    log(\"dims.start\")\n",
        "\n",
        "    # dim_date\n",
        "    date_range = pd.date_range(start=\"2020-01-01\", end=\"2025-12-31\", freq=\"D\")\n",
        "    df_date = pd.DataFrame({\n",
        "        \"date_key\": date_range.date,\n",
        "        \"year\": date_range.year,\n",
        "        \"quarter\": date_range.quarter,\n",
        "        \"month\": date_range.month,\n",
        "        \"month_name\": date_range.strftime(\"%B\"),\n",
        "        \"day_of_month\": date_range.day,\n",
        "        \"day_of_week\": date_range.dayofweek + 1,\n",
        "        \"day_name\": date_range.strftime(\"%A\"),\n",
        "    })\n",
        "    table_id = f\"{PROJECT_ID}.{DATASET_ID}.dim_date\"\n",
        "    bq.load_table_from_dataframe(\n",
        "        df_date,\n",
        "        table_id,\n",
        "        job_config=bigquery.LoadJobConfig(write_disposition=\"WRITE_TRUNCATE\"),\n",
        "    ).result()\n",
        "    log(\"dim_date.loaded\", rows=len(df_date), table=table_id)\n",
        "\n",
        "    # dim_location\n",
        "    sql_location = f\"\"\"\n",
        "    CREATE OR REPLACE TABLE `{PROJECT_ID}.{DATASET_ID}.dim_location` AS\n",
        "    SELECT DISTINCT\n",
        "      CONCAT(Country, \"_\", Region) AS location_key,\n",
        "      Region,\n",
        "      Country\n",
        "    FROM `{PROJECT_ID}.{DATASET_ID}.stg_sales`\n",
        "    UNION DISTINCT\n",
        "    SELECT DISTINCT\n",
        "      CONCAT(Country, \"_\", Region), Region, Country\n",
        "    FROM `{PROJECT_ID}.{DATASET_ID}.stg_finance`\n",
        "    UNION DISTINCT\n",
        "    SELECT DISTINCT\n",
        "      CONCAT(Country, \"_\", Region), Region, Country\n",
        "    FROM `{PROJECT_ID}.{DATASET_ID}.stg_attendance`\n",
        "    \"\"\"\n",
        "    bq.query(sql_location, location=\"US\").result()\n",
        "    log(\"dim_location.created\", table=f\"{PROJECT_ID}.{DATASET_ID}.dim_location\")\n",
        "\n",
        "    # dim_product\n",
        "    sql_product = f\"\"\"\n",
        "    CREATE OR REPLACE TABLE `{PROJECT_ID}.{DATASET_ID}.dim_product` AS\n",
        "    SELECT DISTINCT\n",
        "      CONCAT('PROD_', Product) AS product_key,\n",
        "      Product AS product_name\n",
        "    FROM `{PROJECT_ID}.{DATASET_ID}.stg_sales`\n",
        "    UNION DISTINCT\n",
        "    SELECT DISTINCT\n",
        "      CONCAT('PROD_', Product), Product\n",
        "    FROM `{PROJECT_ID}.{DATASET_ID}.stg_finance`\n",
        "    \"\"\"\n",
        "    bq.query(sql_product, location=\"US\").result()\n",
        "    log(\"dim_product.created\", table=f\"{PROJECT_ID}.{DATASET_ID}.dim_product\")\n",
        "\n",
        "    # dim_employee\n",
        "    sql_employee = f\"\"\"\n",
        "    CREATE OR REPLACE TABLE `{PROJECT_ID}.{DATASET_ID}.dim_employee` AS\n",
        "    SELECT DISTINCT\n",
        "      CONCAT('EMP_', StaffID) AS employee_key,\n",
        "      StaffID,\n",
        "      Name,\n",
        "      Department,\n",
        "      Country AS home_country,\n",
        "      Region  AS home_region\n",
        "    FROM `{PROJECT_ID}.{DATASET_ID}.stg_attendance`\n",
        "    \"\"\"\n",
        "    bq.query(sql_employee, location=\"US\").result()\n",
        "    log(\"dim_employee.created\", table=f\"{PROJECT_ID}.{DATASET_ID}.dim_employee\")\n",
        "\n",
        "    # dim_currency\n",
        "    url = \"https://api.exchangerate-api.com/v4/latest/USD\"\n",
        "    resp = requests.get(url)\n",
        "    resp.raise_for_status()\n",
        "    rates = resp.json()[\"rates\"]\n",
        "    today = datetime.utcnow().date()\n",
        "\n",
        "    df_currency = pd.DataFrame([\n",
        "        {\"currency_key\": code, \"currency_code\": code, \"date_key\": today, \"date_to_usd\": rate}\n",
        "        for code, rate in rates.items()\n",
        "    ])\n",
        "\n",
        "    currency_table = f\"{PROJECT_ID}.{DATASET_ID}.dim_currency\"\n",
        "    bq.load_table_from_dataframe(\n",
        "        df_currency,\n",
        "        currency_table,\n",
        "        job_config=bigquery.LoadJobConfig(write_disposition=\"WRITE_TRUNCATE\"),\n",
        "    ).result()\n",
        "    log(\"dim_currency.loaded\", rows=len(df_currency), table=currency_table)\n",
        "\n",
        "    log(\"dims.done\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uyzgC_F51G5a"
      },
      "source": [
        "Load data into fact_attendance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "executionInfo": {
          "elapsed": 5,
          "status": "ok",
          "timestamp": 1756036254465,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -330
        },
        "id": "QIgjObXHdZ6q"
      },
      "outputs": [],
      "source": [
        "def merge_fact_attendance():\n",
        "    sql = f\"\"\"\n",
        "    MERGE `{PROJECT_ID}.{DATASET_ID}.fact_attendance` T\n",
        "    USING (\n",
        "      SELECT\n",
        "        e.employee_key,\n",
        "        l.location_key,\n",
        "        d.date_key,\n",
        "        s.Status AS status,\n",
        "        -- Already TIME from staging normalization\n",
        "        s.CheckInTime  AS in_time,\n",
        "        s.CheckOutTime AS out_time\n",
        "      FROM `{PROJECT_ID}.{DATASET_ID}.stg_attendance` s\n",
        "      JOIN `{PROJECT_ID}.{DATASET_ID}.dim_employee`  e\n",
        "        ON CAST(s.StaffID AS STRING) = CAST(e.StaffID AS STRING)\n",
        "      JOIN `{PROJECT_ID}.{DATASET_ID}.dim_location`  l\n",
        "        ON s.Country = l.country AND s.Region = l.region\n",
        "      JOIN `{PROJECT_ID}.{DATASET_ID}.dim_date`      d\n",
        "        ON DATE(s.Date) = d.date_key   -- if s.Date is DATE already, you can just do: s.Date = d.date_key\n",
        "    ) S\n",
        "    ON T.employee_key = S.employee_key\n",
        "   AND T.date_key     = S.date_key\n",
        "\n",
        "    WHEN MATCHED THEN UPDATE SET\n",
        "      T.location_key  = S.location_key,\n",
        "      T.status        = S.status,\n",
        "      T.checkin_time  = IF(S.in_time  IS NULL, NULL, TIMESTAMP(DATETIME(S.date_key, S.in_time))),\n",
        "      T.checkout_time = IF(S.out_time IS NULL, NULL, TIMESTAMP(DATETIME(S.date_key, S.out_time)))\n",
        "\n",
        "    WHEN NOT MATCHED THEN INSERT (\n",
        "      attendance_key, employee_key, location_key, date_key, status, checkin_time, checkout_time\n",
        "    ) VALUES (\n",
        "      GENERATE_UUID(), S.employee_key, S.location_key, S.date_key, S.status,\n",
        "      IF(S.in_time  IS NULL, NULL, TIMESTAMP(DATETIME(S.date_key, S.in_time))),\n",
        "      IF(S.out_time IS NULL, NULL, TIMESTAMP(DATETIME(S.date_key, S.out_time)))\n",
        "    );\n",
        "    \"\"\"\n",
        "    run_sql(sql)\n",
        "    log(\"fact_attendance.merged\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CT-Oocti1Ngh"
      },
      "source": [
        "Load data into fact_sales"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "executionInfo": {
          "elapsed": 6,
          "status": "ok",
          "timestamp": 1756036254466,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -330
        },
        "id": "ZBReqGhb05jP"
      },
      "outputs": [],
      "source": [
        "def merge_fact_sales():\n",
        "    sql = f\"\"\"\n",
        "    MERGE `{PROJECT_ID}.{DATASET_ID}.fact_sales` T\n",
        "    USING (\n",
        "      SELECT\n",
        "        CONCAT(s.SaleID) AS nk,\n",
        "        p.product_key, l.location_key, d.date_key,\n",
        "        c.currency_key,\n",
        "        CAST(s.Quantity AS INT64) AS quantity,\n",
        "        CAST(r.date_to_usd AS NUMERIC) AS conversion_rate_to_usd,\n",
        "        CAST(FORMAT('%.2f', ROUND(s.UnitPrice * r.date_to_usd, 2)) AS NUMERIC) AS unit_price_usd,\n",
        "        CAST(FORMAT('%.2f', ROUND(s.TotalSales * r.date_to_usd, 2)) AS NUMERIC) AS total_sales_usd\n",
        "      FROM `{PROJECT_ID}.{DATASET_ID}.stg_sales` s\n",
        "      JOIN `{PROJECT_ID}.{DATASET_ID}.dim_product`  p ON s.Product = p.product_name\n",
        "      JOIN `{PROJECT_ID}.{DATASET_ID}.dim_location` l ON s.Country = l.country AND s.Region = l.region\n",
        "      JOIN `{PROJECT_ID}.{DATASET_ID}.dim_date`     d ON s.Date = d.date_key\n",
        "      JOIN `{PROJECT_ID}.{DATASET_ID}.dim_currency` r ON s.Currency = r.currency_code\n",
        "      JOIN `{PROJECT_ID}.{DATASET_ID}.dim_currency` c ON s.Currency = c.currency_code\n",
        "    ) S\n",
        "    ON T.saleid = S.nk\n",
        "    WHEN MATCHED THEN UPDATE SET\n",
        "      T.product_key = S.product_key,\n",
        "      T.location_key= S.location_key,\n",
        "      T.date_key    = S.date_key,\n",
        "      T.currency_key= S.currency_key,\n",
        "      T.quantity    = S.quantity,\n",
        "      T.conversion_rate_to_usd = S.conversion_rate_to_usd,\n",
        "      T.unit_price_usd = S.unit_price_usd,\n",
        "      T.total_sales_usd= S.total_sales_usd\n",
        "    WHEN NOT MATCHED THEN INSERT (\n",
        "      saleid, product_key, location_key, date_key, currency_key, quantity,\n",
        "      conversion_rate_to_usd, unit_price_usd, total_sales_usd\n",
        "    ) VALUES (\n",
        "      S.nk, S.product_key, S.location_key, S.date_key, S.currency_key, S.quantity,\n",
        "      S.conversion_rate_to_usd, S.unit_price_usd, S.total_sales_usd\n",
        "    );\n",
        "    \"\"\"\n",
        "    run_sql(sql)\n",
        "    log(\"fact_sales.merged\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CL5bgDHI1SO3"
      },
      "source": [
        "Load data into fact_finance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "executionInfo": {
          "elapsed": 6,
          "status": "ok",
          "timestamp": 1756036254466,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -330
        },
        "id": "LkciA9lm05gw"
      },
      "outputs": [],
      "source": [
        "def merge_fact_finance():\n",
        "    sql = f\"\"\"\n",
        "    MERGE `{PROJECT_ID}.{DATASET_ID}.fact_finance` T\n",
        "    USING (\n",
        "      SELECT\n",
        "        CONCAT(f.TransactionID) AS nk,\n",
        "        p.product_key, l.location_key, d.date_key,\n",
        "        c.currency_key,\n",
        "        CAST(r.date_to_usd AS NUMERIC) AS conversion_rate_to_usd,\n",
        "        CAST(FORMAT('%.2f', ROUND(f.Revenue * r.date_to_usd, 2)) AS NUMERIC) AS revenue_usd,\n",
        "        CAST(FORMAT('%.2f', ROUND(f.Expense * r.date_to_usd, 2)) AS NUMERIC) AS expense_usd,\n",
        "        CAST(FORMAT('%.2f', ROUND(f.Profit  * r.date_to_usd, 2)) AS NUMERIC) AS profit_usd\n",
        "      FROM `{PROJECT_ID}.{DATASET_ID}.stg_finance` f\n",
        "      JOIN `{PROJECT_ID}.{DATASET_ID}.dim_product`  p ON f.Product = p.product_name\n",
        "      JOIN `{PROJECT_ID}.{DATASET_ID}.dim_location` l ON f.Country = l.country AND f.Region = l.region\n",
        "      JOIN `{PROJECT_ID}.{DATASET_ID}.dim_date`     d ON f.Date = d.date_key\n",
        "      JOIN `{PROJECT_ID}.{DATASET_ID}.dim_currency` r ON f.Currency = r.currency_code\n",
        "      JOIN `{PROJECT_ID}.{DATASET_ID}.dim_currency` c ON f.Currency = c.currency_code\n",
        "    ) S\n",
        "    ON T.transaction_id = S.nk\n",
        "    WHEN MATCHED THEN UPDATE SET\n",
        "      T.product_key = S.product_key,\n",
        "      T.location_key= S.location_key,\n",
        "      T.date_key    = S.date_key,\n",
        "      T.currency_key= S.currency_key,\n",
        "      T.conversion_rate_to_usd = S.conversion_rate_to_usd,\n",
        "      T.revenue_usd = S.revenue_usd,\n",
        "      T.expense_usd = S.expense_usd,\n",
        "      T.profit_usd  = S.profit_usd\n",
        "    WHEN NOT MATCHED THEN INSERT (\n",
        "      transaction_id, product_key, location_key, date_key, currency_key,\n",
        "      conversion_rate_to_usd,\n",
        "      revenue_usd, expense_usd, profit_usd\n",
        "    ) VALUES (\n",
        "      S.nk, S.product_key, S.location_key, S.date_key, S.currency_key,\n",
        "      S.conversion_rate_to_usd,\n",
        "      S.revenue_usd, S.expense_usd, S.profit_usd\n",
        "    );\n",
        "    \"\"\"\n",
        "    run_sql(sql)\n",
        "    log(\"fact_finance.merged\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R5tw7svC6PPI"
      },
      "source": [
        "Validation and summary report generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "executionInfo": {
          "elapsed": 5,
          "status": "ok",
          "timestamp": 1756036254466,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -330
        },
        "id": "OkV_GoG88BRm"
      },
      "outputs": [],
      "source": [
        "def summarize_and_validate():\n",
        "    checks = {\n",
        "        \"stg_attendance\":  f\"SELECT COUNT(*) c FROM `{PROJECT_ID}.{DATASET_ID}.stg_attendance`\",\n",
        "        \"stg_sales\":       f\"SELECT COUNT(*) c FROM `{PROJECT_ID}.{DATASET_ID}.stg_sales`\",\n",
        "        \"stg_finance\":     f\"SELECT COUNT(*) c FROM `{PROJECT_ID}.{DATASET_ID}.stg_finance`\",\n",
        "        \"fact_attendance\": f\"SELECT COUNT(*) c FROM `{PROJECT_ID}.{DATASET_ID}.fact_attendance`\",\n",
        "        \"fact_sales\":      f\"SELECT COUNT(*) c, MIN(unit_price_usd) min_usd, MAX(unit_price_usd) max_usd FROM `{PROJECT_ID}.{DATASET_ID}.fact_sales`\",\n",
        "        \"fact_finance\":    f\"SELECT COUNT(*) c, MIN(revenue_usd) min_rev, MAX(revenue_usd) max_rev FROM `{PROJECT_ID}.{DATASET_ID}.fact_finance`\",\n",
        "    }\n",
        "    report = {}\n",
        "    for name, sql in checks.items():\n",
        "        rows = list(run_sql(sql))\n",
        "        report[name] = dict(rows[0].items())\n",
        "\n",
        "    print(\"\\n=== SUMMARY REPORT ===\")\n",
        "    print(json.dumps(report, indent=2, default=str))\n",
        "\n",
        "    # USD-only sanity checks\n",
        "    if report[\"fact_sales\"][\"min_usd\"] is None or report[\"fact_finance\"][\"min_rev\"] is None:\n",
        "        raise RuntimeError(\"Validation failed: USD fields contain NULLs.\")\n",
        "    if report[\"fact_sales\"][\"min_usd\"] < 0 or report[\"fact_finance\"][\"min_rev\"] < 0:\n",
        "        raise RuntimeError(\"Validation failed: negative USD encountered.\")\n",
        "    log(\"validation.ok\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dvuvbgp66aOm"
      },
      "source": [
        "Table truncation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "executionInfo": {
          "elapsed": 5,
          "status": "ok",
          "timestamp": 1756036254466,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -330
        },
        "id": "wwOpfnhDwUdf"
      },
      "outputs": [],
      "source": [
        "def truncate_all_tables(project_id: str,\n",
        "                        dataset_id: str,\n",
        "                        prefixes: list[str] | None = None,\n",
        "                        exclude: list[str] | None = None,\n",
        "                        dry_run: bool = False):\n",
        "\n",
        "    client = bigquery.Client(project=project_id)\n",
        "    ds_ref = bigquery.DatasetReference(project_id, dataset_id)\n",
        "    exclude = set(exclude or [])\n",
        "    prefixes = tuple(prefixes or ())\n",
        "\n",
        "    for tbl in client.list_tables(ds_ref):\n",
        "        if tbl.table_type != \"TABLE\":\n",
        "            continue\n",
        "\n",
        "        name = tbl.table_id\n",
        "        if prefixes and not name.startswith(prefixes):\n",
        "            continue\n",
        "        if name in exclude:\n",
        "            continue\n",
        "\n",
        "        sql = f\"TRUNCATE TABLE `{project_id}.{dataset_id}.{name}`\"\n",
        "        if dry_run:\n",
        "            print(sql)\n",
        "        else:\n",
        "            client.query(sql).result()\n",
        "            print(f\"Truncated {name}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qtOiYiMu1kLY"
      },
      "source": [
        "Pipeline driver"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 360706,
          "status": "ok",
          "timestamp": 1756036615167,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -330
        },
        "id": "6Z9sNsHi1cAf",
        "outputId": "b9cbd5a6-502d-4ebe-d5c8-69444c7aa998"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\"ts\": \"2025-08-24T11:50:49.233814Z\", \"step\": \"dataset.exists\", \"dataset\": \"analytics-pipeline-assessment.analytics_dw\"}\n",
            "Truncated dim_currency\n",
            "Truncated dim_employee\n",
            "Truncated dim_location\n",
            "Truncated dim_product\n",
            "Truncated fact_attendance\n",
            "Truncated fact_finance\n",
            "Truncated fact_sales\n",
            "Truncated stg_attendance\n",
            "Truncated stg_attendance_rejects\n",
            "Truncated stg_finance\n",
            "Truncated stg_finance_rejects\n",
            "Truncated stg_sales\n",
            "Truncated stg_sales_rejects\n",
            "{\"ts\": \"2025-08-24T11:51:26.529219Z\", \"step\": \"gen_data.start\", \"order\": \"attendance>sales>finance\"}\n",
            "{\"ts\": \"2025-08-24T11:51:26.529276Z\", \"step\": \"gen.attendance.start\", \"records\": 3000000, \"output\": \"attendance_dataset_3m.csv\"}\n",
            "✅ Attendance dataset generated: attendance_dataset_3m.csv\n",
            "{\"ts\": \"2025-08-24T11:52:19.691135Z\", \"step\": \"gen.attendance.done\", \"file\": \"attendance_dataset_3m.csv\"}\n",
            "{\"ts\": \"2025-08-24T11:52:19.721625Z\", \"step\": \"gen.sales.start\", \"records\": 3000000, \"output\": \"sales_dataset_3m.csv\"}\n",
            "✅ Sales dataset generated: sales_dataset_3m.csv\n",
            "{\"ts\": \"2025-08-24T11:53:04.351211Z\", \"step\": \"gen.sales.done\", \"file\": \"sales_dataset_3m.csv\"}\n",
            "{\"ts\": \"2025-08-24T11:53:04.365725Z\", \"step\": \"gen.finance.start\", \"records\": 3000000, \"output\": \"financial_dataset_3m.csv\"}\n",
            "✅ Finance dataset generated: financial_dataset_3m.csv\n",
            "{\"ts\": \"2025-08-24T11:53:51.623420Z\", \"step\": \"gen.finance.done\", \"file\": \"financial_dataset_3m.csv\"}\n",
            "{\"ts\": \"2025-08-24T11:53:51.640901Z\", \"step\": \"gen_data.done\", \"outputs\": [\"attendance_dataset_3m.csv\", \"sales_dataset_3m.csv\", \"financial_dataset_3m.csv\"]}\n",
            "{\"ts\": \"2025-08-24T11:53:51.640973Z\", \"step\": \"staging.start\"}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-1031560520.py:18: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
            "  parsed = pd.to_datetime(s, errors=\"coerce\").dt.time  # handles HH:MM and HH:MM:SS\n",
            "/tmp/ipython-input-1031560520.py:18: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
            "  parsed = pd.to_datetime(s, errors=\"coerce\").dt.time  # handles HH:MM and HH:MM:SS\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\"ts\": \"2025-08-24T11:54:27.663619Z\", \"step\": \"stg_attendance.validated\", \"total\": 3000000, \"loaded\": 2997754, \"rejected\": 2246}\n",
            "{\"ts\": \"2025-08-24T11:54:27.663813Z\", \"step\": \"staging.attendance.done\", \"rows\": 2997754, \"rejected\": 2246}\n",
            "{\"ts\": \"2025-08-24T11:54:52.596006Z\", \"step\": \"stg_sales.validated\", \"total\": 3000000, \"loaded\": 3000000, \"rejected\": 0}\n",
            "{\"ts\": \"2025-08-24T11:54:52.596152Z\", \"step\": \"staging.sales.done\", \"rows\": 3000000, \"rejected\": 0}\n",
            "{\"ts\": \"2025-08-24T11:55:22.385801Z\", \"step\": \"stg_finance.validated\", \"total\": 3000000, \"loaded\": 3000000, \"rejected\": 0}\n",
            "{\"ts\": \"2025-08-24T11:55:22.385945Z\", \"step\": \"staging.finance.done\", \"rows\": 3000000, \"rejected\": 0}\n",
            "{\"ts\": \"2025-08-24T11:55:22.385979Z\", \"step\": \"staging.all.done\", \"totals\": {\"attendance\": {\"table\": \"stg_attendance\", \"total_rows\": 3000000, \"loaded_rows\": 2997754, \"rejected_rows\": 2246, \"rejects_table\": \"analytics-pipeline-assessment.analytics_dw.stg_attendance_rejects\"}, \"sales\": {\"table\": \"stg_sales\", \"total_rows\": 3000000, \"loaded_rows\": 3000000, \"rejected_rows\": 0, \"rejects_table\": \"analytics-pipeline-assessment.analytics_dw.stg_sales_rejects\"}, \"finance\": {\"table\": \"stg_finance\", \"total_rows\": 3000000, \"loaded_rows\": 3000000, \"rejected_rows\": 0, \"rejects_table\": \"analytics-pipeline-assessment.analytics_dw.stg_finance_rejects\"}}}\n",
            "{\"ts\": \"2025-08-24T11:55:22.386027Z\", \"step\": \"dims.start\"}\n",
            "{\"ts\": \"2025-08-24T11:55:25.334217Z\", \"step\": \"dim_date.loaded\", \"rows\": 2192, \"table\": \"analytics-pipeline-assessment.analytics_dw.dim_date\"}\n",
            "{\"ts\": \"2025-08-24T11:55:27.846421Z\", \"step\": \"dim_location.created\", \"table\": \"analytics-pipeline-assessment.analytics_dw.dim_location\"}\n",
            "{\"ts\": \"2025-08-24T11:55:30.051636Z\", \"step\": \"dim_product.created\", \"table\": \"analytics-pipeline-assessment.analytics_dw.dim_product\"}\n",
            "{\"ts\": \"2025-08-24T11:55:35.299157Z\", \"step\": \"dim_employee.created\", \"table\": \"analytics-pipeline-assessment.analytics_dw.dim_employee\"}\n",
            "{\"ts\": \"2025-08-24T11:55:45.551390Z\", \"step\": \"dim_currency.loaded\", \"rows\": 163, \"table\": \"analytics-pipeline-assessment.analytics_dw.dim_currency\"}\n",
            "{\"ts\": \"2025-08-24T11:55:45.551503Z\", \"step\": \"dims.done\"}\n",
            "{\"ts\": \"2025-08-24T11:55:48.025772Z\", \"step\": \"fact_attendance.merged\"}\n",
            "{\"ts\": \"2025-08-24T11:56:09.754236Z\", \"step\": \"fact_sales.merged\"}\n",
            "{\"ts\": \"2025-08-24T11:56:43.523339Z\", \"step\": \"fact_finance.merged\"}\n",
            "\n",
            "=== SUMMARY REPORT ===\n",
            "{\n",
            "  \"stg_attendance\": {\n",
            "    \"c\": 2997754\n",
            "  },\n",
            "  \"stg_sales\": {\n",
            "    \"c\": 3000000\n",
            "  },\n",
            "  \"stg_finance\": {\n",
            "    \"c\": 3000000\n",
            "  },\n",
            "  \"fact_attendance\": {\n",
            "    \"c\": 0\n",
            "  },\n",
            "  \"fact_sales\": {\n",
            "    \"c\": 3000000,\n",
            "    \"min_usd\": \"74.25\",\n",
            "    \"max_usd\": \"7673838.61\"\n",
            "  },\n",
            "  \"fact_finance\": {\n",
            "    \"c\": 3000000,\n",
            "    \"min_rev\": \"743.03\",\n",
            "    \"max_rev\": \"153476848.91\"\n",
            "  }\n",
            "}\n",
            "{\"ts\": \"2025-08-24T11:56:48.493500Z\", \"step\": \"validation.ok\"}\n",
            "{\"ts\": \"2025-08-24T11:56:48.493538Z\", \"step\": \"pipeline.done\", \"status\": \"success\"}\n"
          ]
        }
      ],
      "source": [
        "def run_pipeline():\n",
        "    try:\n",
        "        ensure_dataset()\n",
        "\n",
        "        truncate_all_tables(PROJECT_ID, DATASET_ID, prefixes=[\"dim_\", \"fact_\", \"stg_\"], exclude=[\"dim_date\"])\n",
        "\n",
        "        # 1) Generate data (attendance → sales → finance)\n",
        "        a_src, s_src, f_src = gen_data()\n",
        "\n",
        "        # 2) Stage\n",
        "        stage_from_generated(a_src, s_src, f_src)\n",
        "\n",
        "        # 3) Dimensions\n",
        "        build_dimensions()\n",
        "\n",
        "        # 4) Facts\n",
        "        merge_fact_attendance()\n",
        "        merge_fact_sales()\n",
        "        merge_fact_finance()\n",
        "\n",
        "        # 5) Validate and report\n",
        "        summarize_and_validate()\n",
        "\n",
        "        log(\"pipeline.done\", status=\"success\")\n",
        "    except Exception as e:\n",
        "        log(\"pipeline.failed\", error=str(e))\n",
        "        raise\n",
        "\n",
        "# Execute\n",
        "run_pipeline()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Crq95YZ94dHh"
      },
      "source": [
        "Download generated input files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "executionInfo": {
          "elapsed": 4,
          "status": "ok",
          "timestamp": 1756036615167,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -330
        },
        "id": "D7dejwRuDwU0"
      },
      "outputs": [],
      "source": [
        "# from google.colab import files\n",
        "\n",
        "# files.download('attendance_dataset_3m.csv')\n",
        "# files.download('sales_dataset_3m.csv')\n",
        "# files.download('financial_dataset_3m.csv')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Notebook2",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
